---
cinder_iscsi_helper: tgtadm
cinder_iscsi_iotype: fileio
cinder_iscsi_num_targets: 100
cinder_iscsi_port: 3260

## Ceph cluster fsid (must be generated before first run)
## Generate a uuid using: python -c 'import uuid; print(str(uuid.uuid4()))'
generate_fsid: false
fsid: 32d556e2-3a78-4a90-8e89-5ccd7bd6bef0

# Not sure why this has to be defined again if it has already been defined
# in openstack_user_config.yml
cidr_networks:
  container: 172.29.236.0/24
  tunnel: 172.29.240.0/24
  storage: 172.29.244.0/24

## ceph-ansible settings
## See https://github.com/ceph/ceph-ansible/tree/master/group_vars for
## additional configuration options available.
monitor_address_block: "172.29.236.0/24"
public_network: "172.29.236.0/24"
cluster_network: "172.29.244.0/24"
journal_size: 10240 # size in MB
# ceph-ansible automatically creates pools & keys for OpenStack services
openstack_config: true
cinder_ceph_client: cinder
glance_ceph_client: glance
glance_default_store: rbd
glance_rbd_store_pool: images
nova_libvirt_images_rbd_pool: vms

ceph_repository: community
ceph_mirror: http://download.ceph.com
ceph_stable_key: https://download.ceph.com/keys/release.asc
ceph_stable_release: nautilus
ceph_stable_repo: "{{ ceph_mirror }}/debian-{{ ceph_stable_release }}"

# Devices to be used with Ceph
devices:
  - /dev/vdb
  - /dev/vdc
  - /dev/vdd

# For swift
# global_overrides:
#   swift:
#     part_power: 7
#     storage_network: br-storage
#     replication_network: br-storage
#     drives:
#       - name: vdb
#       - name: vdc
#       - name: vdd
#       - name: vde
#       - name: vdf
#     mount_point: /srv
#     storage_policies:
#       - policy:
#           name: default
#           index: 0
#           default: True
